{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing student performance in an online course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will assess the risk of a student failing a course module based on student characterstics (gender, age, etc.) and information about their activity (studied credits, number of previous attempts to pass the course). To do that, we will train a student model using logistic regression.\n",
    "\n",
    "Then, we will try to improve the model's performance in terms of accuracy by using the assignments' grades as an additional factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the python libraries that we will need for our analysis\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Future Warning message\n",
    "# Reference:\n",
    "# https://stackoverflow.com/questions/40659212/futurewarning-elementwise-comparison-failed-returning-scalar-but-in-the-futur\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the needed data into dataframes.\n",
    "Here we assume that the data files are in the same directory as the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read student information\n",
    "studentInfo = pd.read_csv(\"studentInfo.csv\") \n",
    "#print out the 10 first rows of the data\n",
    "studentInfo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see all potential final results\n",
    "studentInfo[\"final_result\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column to classify final results. classify studets with a pass or distinction as \"1\", the rest as \"0\"\n",
    "studentInfo[\"result.class\"] = 1\n",
    "\n",
    "#studentInfo[\"result.class\"] = studentInfo[\"final_result\"].apply(lambda x: 0 if (x == 'Fail') | x == \"Withdrawn\") else 1)\n",
    "studentInfo[\"result.class\"].loc[(studentInfo[\"final_result\"] == \"Withdrawn\") | (studentInfo[\"final_result\"] == \"Fail\")] = 0\n",
    "\n",
    "#and look at the dataset again\n",
    "studentInfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create one dataframe (Xfactors) with all the factors (variables) that we will use to assess whether a student will pass of fail the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfactors = studentInfo[[\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"num_of_prev_attempts\", \"studied_credits\", \"disability\"]]\n",
    "X_noncat = pd.get_dummies(Xfactors)\n",
    "\n",
    "X_noncat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create another variable (Youtcome) which represents the outcome, that is what we want to assess.\n",
    "Here, we want to assess whether a student will pass the course successfully or not - which is represented by the variable \"result.class\".\n",
    "Please remember, 1 means the student passes the course, 0 means the student fails the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Youtcome = studentInfo[\"result.class\"].values\n",
    "Youtcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to fit our model! This means that we will use \"old\" data - where we already know the outcome - to train the model. We will also keep a part of the old data to test our model's performance - that is whether the model learned to an acceptable degree to assess student performance.\n",
    "The datasets used for training have the suffix \"_train\" while the datasets saved for testing have the suffix \"_test\".\n",
    "The model is trained as a logistic regression binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noncat, Youtcome, test_size=0.3, random_state=0)\n",
    "OurModel = LogisticRegression()\n",
    "OurModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use our model (OurModel) to assess student performance using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on a testset\n",
    "y_pred = OurModel.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(OurModel.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results, our model can assess student performance correctly with a 61% accuracy. \n",
    "This is not really good, is it? \n",
    "Lets try to improve the accuracy by adding one more variable to the predictive features (the Xfactors dataframe): the students'average grade of the Teacher Marked Asssessments (TMA) assignments of the course.\n",
    "\n",
    "To do that, we will need the data contained in the tables: assessment and studentAssessment.\n",
    "The analysis follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read additional data\n",
    "studentAssessments = pd.read_csv(\"studentAssessment.csv\")\n",
    "assessments = pd.read_csv(\"assessments.csv\")\n",
    "\n",
    "# studentAssessments.head(10)\n",
    "# assessments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the ids only of the teacher assessments (TMA)\n",
    "TAM = assessments.loc[assessments['assessment_type'] == \"TMA\"]\n",
    "\n",
    "\n",
    "#then keep the students assessments (grades) that were only given by the teacher (TAM) and remove unknown entries (\"?\")\n",
    "TAM_student_grades = studentAssessments.loc[studentAssessments.id_assessment.isin(TAM[\"id_assessment\"])]\n",
    "TAM_student_grades = TAM_student_grades.loc[TAM_student_grades['score'] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty list where we will save the average grade for each and every student\n",
    "avg_grades = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studentInfo['id_student'] = studentInfo['id_student'].fillna(0)\n",
    "studentInfo[\"id_student\"].un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each student find all TMA scores for the course we are interested, and get the mean value\n",
    "\n",
    "for i in range (0, len(studentInfo['id_student'])):\n",
    "    \n",
    "    this_student = studentAssessments.loc[(studentAssessments['id_student'] == studentInfo['id_student'][i]) &\n",
    "                                          (studentAssessments['score'] != '?')]\n",
    "    \n",
    "    assmt = list(this_student['id_assessment'])\n",
    "    score = list(this_student['score'].astype(float)) \n",
    "    # must be converted to 'float' instead of 'int'\n",
    "    # reference: \n",
    "    # https://stackoverflow.com/questions/41550746/error-using-astype-when-nan-exists-in-a-dataframe/41550787\n",
    "    \n",
    "    final_score = 0\n",
    "    for j in range(0, len(assmt)):\n",
    "        idx = assessments.loc[assessments.id_assessment == assmt[j]].index[0]\n",
    "        if((assessments.code_module[idx] == studentInfo['code_module'][i]) & (assessments.assessment_type[idx] == \"TMA\")):\n",
    "            final_score = final_score + (float(assessments.weight[idx])*score[j])/100\n",
    "            \n",
    "    avg_grades.append(final_score)\n",
    "    \n",
    "#add the new information about average TAM grades to the student information dataframe\n",
    "\n",
    "studentInfo['avg_TMA_assessment'] = avg_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the new information about average TAM grades to our model\n",
    "\n",
    "Xfactors_updated = studentInfo[[\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"num_of_prev_attempts\", \"studied_credits\", \"disability\", \"avg_TMA_assessment\"]]\n",
    "X_noncat_updated = pd.get_dummies(Xfactors_updated)\n",
    "\n",
    "X_noncat_updated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_noncat_updated, Youtcome, test_size=0.3, random_state=0)\n",
    "OurModelUpdated = LogisticRegression()\n",
    "OurModelUpdated.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the updated model (OurModelUpdated) to assess student performance using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on a testset\n",
    "y_pred = OurModelUpdated.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(OurModelUpdated.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, what about that! The model now can assess students' performance with 83% accuracy!\n",
    "Looks like the grades of Teacher Marked Assignments really helped us to improve the performance of our model :)\n",
    "I wonder what else could help.... ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next time....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to evaluate the models and choose \"THE BEST\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
