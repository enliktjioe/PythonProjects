{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"150\">\n",
    "<br>\n",
    "\n",
    "**Coursebook: Convolutional Neural Network Basic**\n",
    "\n",
    "- Course Length: 3 Hours\n",
    "- Last Update: February 2022\n",
    "\n",
    "___\n",
    "\n",
    "Develop by Dwi Gustin Nurdialit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coursebook is  prepared by [Algoritma](https://algorit.ma). The coursebook is intended for a restricted audience only, i.e. the individuals and organizations having received this coursebook directly from the training organization. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.\n",
    "\n",
    "Algoritma is a data science education center based in Jakarta. We organize workshops and training programs to help working professionals and students gain mastery in various data science sub-fields: data visualization, machine learning, data modeling, statistical inference etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "\n",
    "## Background\n",
    "\n",
    "Casting is a manufacturing process in which liquid material is poured into a mold to solidify. Many types of defects or unwanted irregularities can occur during this process. The industry has its quality inspection department to remove defective products from the production line, but this is very time consuming since it is carried out manually. Furthermore, there is a chance of misclassifying due to human error, causing rejection of the whole product order. \n",
    "\n",
    "From these problems we will use image data to create a machine learning model. One of the most popular and good methods for working with image datasets is the Convolutional Neural Network (CNN).\n",
    "\n",
    "This material aims to provide an understanding to the workshop participants to automate the inspection process by training top-view images of a casted submersible pump impeller using Convolutional Neural Network (CNN). One Instructor and two Teaching Assistants will help participants troubleshoot or help with any difficulties encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "\n",
    "The objective of this course is to provide participants with a comprehensive introduction to understanding deep learning model building for image data using Python. We will learn techniques on how to process image data to obtain a deep learning model. Syllabus includes:\n",
    "\n",
    "- **Convolutional Neural Networks:**\n",
    "    + Convolution concept: kernel convolutinals, strides, padding, and filter\n",
    "    + Convolutional Neural Network Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "# Convolutional Neural Network\n",
    "\n",
    "Convolutional Neural Network are nowadays standard architecture to deal with image data. Its history started in 1989 when Yan Lecunn created its first Optical Character Recognition (OCR) model to classify numbers and characters. But due to the flaw of deep neural network activation functions, most of the network will fails of either vanishing gradient or exploding gradient. The problem occurs until in 2011, Yoshua Bengio created Rectified Linear Unit that enables most of deep learning architecture to avoid the problems. \n",
    "\n",
    "A year after, Alex Krizhevsky re-created Yann Lecunn's CNN and implmented Relu in it. The model was submitted on 2012 Imagenet Competition and the performance was way better than any Deep Neural Network architecture at that time. The model then become the first heavily known CNN that works and its named upon its author, \"Alex Net\". It marked as the starting point of Deep Learning hype for computer vision. \n",
    "\n",
    "## Convolution Concepts\n",
    "\n",
    "In previous module, we learned to classify a hand-digit image into its classes. If you take a look closely, you might notice that there are so many empy space that actually doesn't affect the classification much. All those irrelevant pixels get in anyway into our neural network models, burdening the network with huge feature that are irrelevant to the class and makes it harder to train. \n",
    "\n",
    "What if we could extract those relevant values only and remove all the irrelevant pixels? That way our network will have so much lighter feature but with relatively same (or even better) information. This is when convolution takes part. Please take a look at convolutional neural network architecture below : \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"assets/cnn-general.jpeg\" width=\"90%\">\n",
    "<!--     <figcaption> Fig.1 - Illustration of Convolutional Neural Network Atchitecture </figcaption> -->\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "If you take a close look, the end of the network was basically a neural network. Nowadays, these layers are called dense (since all the node are densely connected). Based on the illustration, the convolution part was used to extract important feature from the data before being fed into a dense layers. The convoluted data might be **smaller** in size but **richer** in information, resulting in more effective works for the dense layers. \n",
    "\n",
    "There are two main activity in the convolution, firstly the convolution itself, and the second is pooling. \n",
    "\n",
    "\n",
    "### 1. Convolution\n",
    "\n",
    "To briefly understand how the convolution works, please take alook at below picture of animal then make a guess what kind of animal it is.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"assets/tasmanian.jpg\" width=\"35%\">\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Have you make your guess? You probably went for cat, rat, or even a bear. Whatever you have guessed, the interesting part is the process of guessing. There should be some physical characteristic from the image that snaps your brain to think like \"Oh, this particular part of the body looks like cat\", and that's just how our brain works. It remember particular parts of an object. In fact, it really goods at finding pattern and visual characteristics. If we are given something we don't know, our brain will certainly think of something similar in the past. This is something that we are going to try to mimic using convolution process. \n",
    "\n",
    "A convolution will extract meaningful information from the data using filters. These filters works just like any filter in real world, it has specific usage and has sensitivity over a very specific means. For eaxmple, think of a UV filter for camera lens. It will block UV lights to reduce the excesssive blue color from the sky. The more UV light on the field, the more this filter will active to tell you that there are UV lights. \n",
    "\n",
    "Mathematically, the feedforward process of convolutional neural network is called \"cross correlation\". The term convolution comes from its derivative function when the network do backpropagation. Below are the illustraion and mathematical formula on how the network do feedforwards\n",
    "\n",
    "\n",
    "$$ F \\circ I (x,y) = \\sum_{j=-N}^{N} \\sum_{i=-N}^{N} F(i,j) \\times I(x+i, y+j)$$\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"assets/conv-hackernoon.gif\" width=\"50%\">\n",
    "<!--     <figcaption> Fig.2 - Convolution Illustration by Hackernoon</figcaption> -->\n",
    "    </center>\n",
    "</figure>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, the convolution implementation lays under the `keras.layes.Conv2D` class. Below are example of creating the layer then attach it in the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single layer of convolution\n",
    "my_conv_layer = keras.layers.Conv2D(\n",
    "    input_shape = (28,28,1),\n",
    "    filters = 5,  # Number of filter\n",
    "    kernel_size = 3, # Filter size\n",
    "    strides = 1, # Steps regarding the convolution \n",
    "    padding = 'same',\n",
    "    activation = 'relu', # Default : no activation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code, the parameters will works as follows:\n",
    "- **filters** will determine how many number of filter will be used to convolve the image. The more the filter, the more likely it is to learn more specific features. This number is equivalent with number of neuron in a dense layer (`units`)\n",
    "- **kernel_size** will determine each filter size. Larger feature will catch more information and most likely generalize better than smaller ones. But study shows that kernel size of 3 and 5 is powerful in terms of algorithms complexity. Tho, there is no strict standard in determining the kernel size. The best practice is to use a small odd values. \n",
    "- **strides** acts as steps in moving the filter during the convolutoin process. Large strides will make the steps larger and make the filter potentially missed some meaningful feature (pixels)\n",
    "- **padding** is added if we wanted the output size to be same with the input by doing some padding according to the filter size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how the convolutional layer works, we will try to feed from mnist digit dataset. The shape will be `(batch_size, dim1, dim2)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type \t:  <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "training size \t:  (60000, 28, 28) (60000,)\n",
      "Test size \t:  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "with np.load('data_input/mnist.npz') as data:\n",
    "  x_train = data['x_train']\n",
    "  y_train = data['y_train']\n",
    "  x_test = data['x_test']\n",
    "  y_test = data['y_test']\n",
    "\n",
    "# Check type and shape\n",
    "print('data type \\t: ', type(x_train), type(y_train))\n",
    "print('training size \\t: ', x_train.shape, y_train.shape)\n",
    "print('Test size \\t: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the dimension then transform the data type as float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = x_train[0].reshape(-1,28,28,1).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tenshorshape of [1,28,28,1] determines the image size of 28x28x1. To see how the sample image looks like, we can visualize the first element of `x_train[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[0]\n",
    "plt.imshow(image, cmap='gray');\n",
    "print('Label:', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dummy data is ready, we can feed the layer with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 11:37:01.372624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 28, 28, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedforward x through my_conv_layer\n",
    "output_conv = my_conv_layer(sample_image)\n",
    "output_conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above:\n",
    "\n",
    "- The first dimension of the output represents the number of data or images that being fed in (batch size)\n",
    "- The last dimension represents the number of filters exists in the particular layers. Any other dimension between the two is calculated as we previously presented in convolution inllustration. \n",
    "\n",
    "To make a grasp on how a filter process the data, below is an illustration on how the first filter will output once the first image is fed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOD0lEQVR4nO3df6yeZX3H8fdnFNoamVQ7pSlVJGvcmFsCNoiymGZigo2hJnYJ/KFgJGc6YbposqoJJiZz6B8uMxpIA0RYDJKp0eNSY3DAcFlgVFIohWALZqG1EwQpEkFW990f58Y8Hs6vXs99nuc5+H4lT57rvu/r3Ne3V5tP759tqgpJOl6/N+4CJK1MhoekJoaHpCaGh6QmhoekJoaHpCZDhUeSVya5JcmB7nvdPP1+nWRv95keZkxJkyHDPOeR5PPAk1V1VZKdwLqq+rs5+j1TVS8fok5JE2bY8HgI2FpVR5JsAG6vqjfM0c/wkF5ihg2Pp6rqlK4d4OcvLM/qdwzYCxwDrqqqb82zvylgCmDVqlVvWrduzrMgAY8//vi4S5h4GzduHHcJE+/w4cM/q6o/aPnZVYt1SPJ94NQ5Nn1qcKGqKsl8SfS6qjqc5Azg1iT7qurh2Z2qahewC+DVr3517dixY9FfwO+qq6++etwlTLwrrrhi3CVMvJ07d/53688uGh5Vdf5825L8NMmGgdOWx+bZx+Hu+5EktwNnAS8KD0krx7C3aqeBS7r2JcC3Z3dIsi7J6q69HjgPeGDIcSWN2bDhcRXwjiQHgPO7ZZJsSXJt1+ePgT1J7gVuY+aah+EhrXCLnrYspKqeAN4+x/o9wGVd+z+BPx1mHEmTxydMJTUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNeklPJJckOShJAeT7Jxj++okN3fb70pyeh/jShqfocMjyQnAl4F3AmcCFyc5c1a3DwA/r6o/BP4R+Nyw40oarz6OPM4BDlbVI1X1PPA1YPusPtuBG7r214G3J0kPY0sakz7CYyPw6MDyoW7dnH2q6hhwFHhVD2NLGpOJumCaZCrJniR7nn322XGXI2kBfYTHYWDTwPJp3bo5+yRZBbwCeGL2jqpqV1Vtqaota9eu7aE0Sculj/C4G9ic5PVJTgIuAqZn9ZkGLunaO4Bbq6p6GFvSmKwadgdVdSzJ5cD3gBOA66tqf5LPAHuqahq4DvjnJAeBJ5kJGEkr2NDhAVBVu4Hds9ZdOdB+DvjLPsaSNBkm6oKppJXD8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNSkl/BIckGSh5IcTLJzju2XJnk8yd7uc1kf40oan1XD7iDJCcCXgXcAh4C7k0xX1QOzut5cVZcPO56kydDHkcc5wMGqeqSqnge+BmzvYb+SJtjQRx7ARuDRgeVDwJvn6PeeJG8DfgT8bVU9OrtDkilgCuBlL3sZTz31VA/lvTRV1bhLmHgHDhwYdwkTb+fOF11lWLJRXTD9DnB6Vf0ZcAtww1ydqmpXVW2pqi1r1qwZUWmSWvQRHoeBTQPLp3XrfqOqnqiqX3WL1wJv6mFcSWPUR3jcDWxO8vokJwEXAdODHZJsGFi8EHiwh3EljdHQ1zyq6liSy4HvAScA11fV/iSfAfZU1TTwN0kuBI4BTwKXDjuupPHq44IpVbUb2D1r3ZUD7U8An+hjLEmTwSdMJTUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNeklPJJcn+SxJPfPsz1JvpjkYJL7kpzdx7iSxqevI4+vABcssP2dwObuMwVc3dO4ksakl/CoqjuAJxfosh24sWbcCZySZEMfY0saj1Fd89gIPDqwfKhb91uSTCXZk2TPc889N6LSJLWYqAumVbWrqrZU1ZY1a9aMuxxJCxhVeBwGNg0sn9atk7RCjSo8poH3dXddzgWOVtWREY0taRms6mMnSW4CtgLrkxwCPg2cCFBV1wC7gW3AQeCXwPv7GFfS+PQSHlV18SLbC/hwH2NJmgwTdcFU0spheEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIatJLeCS5PsljSe6fZ/vWJEeT7O0+V/YxrqTx6eU/uga+AnwJuHGBPj+oqnf1NJ6kMevlyKOq7gCe7GNfklaGvo48luItSe4FfgJ8vKr2z+6QZAqYAli7di3PP//8CMtbWT772c+Ou4SJ9+Mf/3jcJbykjSo87gFeV1XPJNkGfAvYPLtTVe0CdgGsW7euRlSbpAYjudtSVU9X1TNdezdwYpL1oxhb0vIYSXgkOTVJuvY53bhPjGJsScujl9OWJDcBW4H1SQ4BnwZOBKiqa4AdwIeSHAOeBS6qKk9LpBWsl/CoqosX2f4lZm7lSnqJ8AlTSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTQwPSU0MD0lNDA9JTYYOjySbktyW5IEk+5N8ZI4+SfLFJAeT3Jfk7GHHlTReffxH18eAj1XVPUlOBn6Y5JaqemCgzzuBzd3nzcDV3bekFWroI4+qOlJV93TtXwAPAhtnddsO3Fgz7gROSbJh2LEljU+v1zySnA6cBdw1a9NG4NGB5UO8OGAkrSB9nLYAkOTlwDeAj1bV0437mAKmANauXdtXaZKWQS9HHklOZCY4vlpV35yjy2Fg08Dyad2631JVu6pqS1VtWb16dR+lSVomfdxtCXAd8GBVfWGebtPA+7q7LucCR6vqyLBjSxqfPk5bzgPeC+xLsrdb90ngtQBVdQ2wG9gGHAR+Cby/h3EljdHQ4VFV/wFkkT4FfHjYsSRNDp8wldTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNRk6PBIsinJbUkeSLI/yUfm6LM1ydEke7vPlcOOK2m8VvWwj2PAx6rqniQnAz9McktVPTCr3w+q6l09jCdpAgx95FFVR6rqnq79C+BBYOOw+5U02VJV/e0sOR24A3hjVT09sH4r8A3gEPAT4ONVtX+On58CprrFNwL391ZcP9YDPxt3EQOsZ2GTVg9MXk1vqKqTW36wt/BI8nLg34G/r6pvztr2+8D/VdUzSbYB/1RVmxfZ356q2tJLcT2ZtJqsZ2GTVg9MXk3D1NPL3ZYkJzJzZPHV2cEBUFVPV9UzXXs3cGKS9X2MLWk8+rjbEuA64MGq+sI8fU7t+pHknG7cJ4YdW9L49HG35TzgvcC+JHu7dZ8EXgtQVdcAO4APJTkGPAtcVIufL+3qoba+TVpN1rOwSasHJq+m5np6vWAq6XeHT5hKamJ4SGoyMeGR5JVJbklyoPteN0+/Xw885j69DHVckOShJAeT7Jxj++okN3fb7+qebVlWS6jp0iSPD8zLZctYy/VJHksy5zM4mfHFrtb7kpy9XLUcR00jez1iia9rjHSOlu0VkqqaiA/weWBn194JfG6efs8sYw0nAA8DZwAnAfcCZ87q89fANV37IuDmZZ6XpdR0KfClEf0+vQ04G7h/nu3bgO8CAc4F7pqAmrYC/zqi+dkAnN21TwZ+NMfv10jnaIk1HfccTcyRB7AduKFr3wC8eww1nAMcrKpHqup54GtdXYMG6/w68PYXbkOPsaaRqao7gCcX6LIduLFm3AmckmTDmGsamVra6xojnaMl1nTcJik8XlNVR7r2/wCvmaffmiR7ktyZ5N0917AReHRg+RAvnuTf9KmqY8BR4FU913G8NQG8pzsE/nqSTctYz2KWWu+ovSXJvUm+m+RPRjFgd0p7FnDXrE1jm6MFaoLjnKM+nvNYsiTfB06dY9OnBheqqpLMdw/5dVV1OMkZwK1J9lXVw33XusJ8B7ipqn6V5K+YOTL6izHXNEnuYebPzQuvR3wLWPD1iGF1r2t8A/hoDbznNU6L1HTcczTSI4+qOr+q3jjH59vAT184dOu+H5tnH4e770eA25lJ0b4cBgb/1j6tWzdnnySrgFewvE/LLlpTVT1RVb/qFq8F3rSM9SxmKXM4UjXi1yMWe12DMczRcrxCMkmnLdPAJV37EuDbszskWZdkdddez8zTrbP/3ZBh3A1sTvL6JCcxc0F09h2dwTp3ALdWd8VpmSxa06zz5QuZOacdl2ngfd0dhXOBowOno2OREb4e0Y2z4OsajHiOllJT0xyN4gr0Eq8Ivwr4N+AA8H3gld36LcC1XfutwD5m7jjsAz6wDHVsY+Zq9MPAp7p1nwEu7NprgH8BDgL/BZwxgrlZrKZ/APZ383Ib8EfLWMtNwBHgf5k5V/8A8EHgg932AF/uat0HbBnB/CxW0+UD83Mn8NZlrOXPgQLuA/Z2n23jnKMl1nTcc+Tj6ZKaTNJpi6QVxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDU5P8BbQMLiAKtYcMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(my_conv_layer.weights[0][:,:,:,0], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_filter1 = output_conv[:,:,:,0]\n",
    "plt.imshow(output_filter1[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the resulting images we can actually see a representation object on what actually the filters learns. You can see the example from the article of [visualizing CNN filters](https://cs231n.github.io/understanding-cnn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pooling\n",
    "\n",
    "If we have convoluted feature extracted from the convolutions process, it may somehow still consists redundant or slightly insignifficant features. Not to mention that the size might explode. The idea of pooling is to summarize and simplify the convolved feature by doing aggreagation over the convolved feature. Remember that we wanted the dense layer to be fed with small yet meaningful features. Below are example of Max Pooling where the convolved feature being summarized into a 2x2 data. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"assets/maxpool_animation.gif\" width=\"50%\">\n",
    "<!--     <figcaption> Fig.3 - Max Pooling Illustration by Google ML Practicum</figcaption> -->\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Just like we have convolutional layer, we can try to make and see how the pooling layers works with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pool_layer = keras.layers.MaxPooling2D(\n",
    "    pool_size = 2, # Pool size 2x2\n",
    "    strides = 2, \n",
    "    padding = 'valid'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pool_size` parameter is equivalent with `kernel_size` that will determine how big your pool is. Besides MaxPooling, there are several built-ins function to help you reduce the feature: \n",
    "\n",
    "- MaxPooling1D layer\n",
    "- MaxPooling2D layer\n",
    "- MaxPooling3D layer\n",
    "- AveragePooling1D layer\n",
    "- AveragePooling2D layer\n",
    "- AveragePooling3D layer\n",
    "- GlobalMaxPooling1D layer\n",
    "- GlobalMaxPooling2D layer\n",
    "- GlobalMaxPooling3D layer\n",
    "- GlobalAveragePooling1D layer\n",
    "- GlobalAveragePooling2D layer\n",
    "- GlobalAveragePooling3D layer\n",
    "\n",
    "All above functions are available on `keras.layers` module. However, for advanced reasearch you can also create your own pooling function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CNN in Keras for Training Data\n",
    "\n",
    "In this section we will implement CNN on MNIST digit classification case. We have provided an example to build the model. Your task is to **create your own model** and make a good performance within 100 epochs. But first, prepare your data by re-read the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Image data: Scaling & Reshaping**\n",
    "\n",
    "Before create CNN model, we will preprocess the data by training it into the shape the model expects and scaling it to that all valued are in the `[0,1]` interval. Previously our training images were stored in an array of shape `(60000, 28, 28)`. We will transform it into an array of shape `(60000, 28, 28, 1)` with values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size \t:  (60000, 28, 28, 1) (60000, 10)\n",
      "Test size \t:  (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "x_train=x_train.reshape(60000,28,28,1)\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "x_test=x_test.reshape(10000,28,28,1)\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print('training size \\t: ', x_train.shape, y_train.shape)\n",
    "print('Test size \\t: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create CNN Model** \\\n",
    "Below we will create architecture, complete the code to make the network of your own before we train it. You are free to discuss on how should you build the model.\n",
    "\n",
    "<!--  Reference\n",
    "# Answer\n",
    "# Model Init\n",
    "model_cnn_1=keras.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_cnn_1.add(keras.layers.InputLayer(input_shape=(28,28,1)))\n",
    "\n",
    "# Convolution Layer\n",
    "model_cnn_1.add(keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    strides = 1,\n",
    "    activation = 'relu'\n",
    "))\n",
    "\n",
    "model_cnn_1.add(keras.layers.Conv2D(\n",
    "    filters = 128,\n",
    "    kernel_size = 5,\n",
    "    strides = 1,\n",
    "    activation = 'relu'\n",
    "))\n",
    "\n",
    "# Pooling Layer\n",
    "model_cnn_1.add(keras.layers.MaxPooling2D(\n",
    "    pool_size = 2,\n",
    "    strides = 1,\n",
    "    padding = 'valid'\n",
    "))\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "model_cnn_1.add(keras.layers.Flatten())\n",
    "model_cnn_1.add(keras.layers.Dense(units = 32, activation = 'relu'))\n",
    "model_cnn_1.add(keras.layers.Dense(units = 24, activation = 'relu'))\n",
    "model_cnn_1.add(keras.layers.Dense(units = 16, activation = 'relu'))\n",
    "\n",
    "# Ouput Layer\n",
    "model_cnn_1.add(keras.layers.Dense(units = 10, activation = 'softmax'))\n",
    "model_cnn_1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Init\n",
    "model_cnn=keras.models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model_cnn.add(keras.layers.InputLayer(input_shape=(___)))\n",
    "\n",
    "# Convolution Layer\n",
    "model_cnn.add(keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    strides = 1,\n",
    "    activation = '___'\n",
    "))\n",
    "\n",
    "model_cnn.add(keras.layers.Conv2D(\n",
    "    filters = 128,\n",
    "    kernel_size = 5,\n",
    "    strides = 1,\n",
    "    activation = '___'\n",
    "))\n",
    "\n",
    "# Pooling Layer\n",
    "model_cnn.add(keras.layers.MaxPooling2D(\n",
    "    pool_size = 2,\n",
    "    strides = 1,\n",
    "    padding = 'valid'\n",
    "))\n",
    "\n",
    "\n",
    "# Dense Layer\n",
    "model_cnn.add(keras.layers.Flatten())\n",
    "model_cnn.add(keras.layers.Dense(units = ___, activation = '___'))\n",
    "model_cnn.add(keras.layers.Dense(units = ___, activation = '___'))\n",
    "model_cnn.add(keras.layers.Dense(units = ___, activation = '___'))\n",
    "\n",
    "# Ouput Layer\n",
    "model_cnn.add(keras.layers.Dense(units = ___, activation = '___'))\n",
    "model_cnn.compile(optimizer='___', loss='___', metrics=['___'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**External Links**:\n",
    "\n",
    "- [Adam](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "- [HDF5 Data Type](https://support.hdfgroup.org/HDF5/doc1.6/UG/11_Datatypes.html)\n",
    "- [Keras Sequential Model](https://keras.io/guides/sequential_model)\n",
    "- [Keras Dense Layer](https://keras.io/api/layers/core_layers/dense/)\n",
    "- [L1 vs L2 Regularization](https://explained.ai/regularization/L1vsL2.html)\n",
    "- [Neuron Transmits Messages In The Brain - Genetic Science Learning Center](http://learn.genetics.utah.edu/content/neuroscience/neurons/)\n",
    "- [Relu](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
    "- [RMSProp](https://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telkomathon3",
   "language": "python",
   "name": "telkomathon3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
